\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsfonts}
\usepackage{amsmath}

\font\myfont=cmr12 at 20pt
\title{{\myfont Replication of Token Merging for fast Stable Diffusion}}
\author{Henri Balster}
\date{}

\begin{document}

\maketitle

\tableofcontents

\section*{Abstract}
Einleitender Text.





\newpage
\section{Introduction}
In this work, our goal is to speed up an off-the-shelf Sta-
ble Diffusion model without training using ToMe
Token Merging \cite{bolya2023tomesd}





\newpage
\section{Background}
\subsection{Tokens}
Text.

\subsection{Stable Diffusion}
Text.

\subsubsection{Transformer Architecture}
Text.

\subsubsection{Attention and Self-Attention}
Text.

\subsection{Frechet-Inception-Distance (FID)}
PyTorch implementation \cite{Seitzer2020FID}

\subsubsection{Inception Model}
Text.





\newpage
\section{Related Work}

\subsection{Token Pruning}
Text.





\newpage
\section{Token Merging}
In Token Merging for Stable Diffusion\cite{bolya2023tomesd} the number of tokens is reduced by r\% by merging similar tokens before every diffusion step and unmerging them after to retain the original size of the image.\\ The tokens are partitioned into a source (src) and a destination (dst) set and the most similar tokens from the src set are continously merged into their dst counterparts until the number of tokens has reduced by \(r\)\%, with \(r\) being a hyperparameter determined by the user.\\ The choice of \(r\) is a trade-off between image fidelity and diffusion time as a lower amount of tokens requires a smaller computation time but more information about the image is lost in merge process.\\
The default setup of tomesd\cite{bolya2023tomesd} only applies merging to the self-attention layer in the transformer.

\subsection{Merge and Unmerge algorithms}
\subsubsection*{Merging.} The Merge algorithm uses Bipartite-Soft-Matching to determine the similarity of tokens between the src and dst set. The two most similar tokens are taken and merged into a new token until the overall number of tokens has reduced by \(r\)\%.\\
Two tokens with \(c\) channels \(x_1, x_2  \in \mathbb{R}^c\) would be merged into a new token \(x_{1,2}^* \in \mathbb{R}^c \) by averaging it's features, e.g. \[x_{1,2}^* = \frac{x_1 + x_2}{2}\]

\subsubsection*{Unmerging.} The Unmerge algorithm takes an originally merged token $x_{1,2}^* \in \mathbb{R}^c$ and splits it up into its original tokens $x_1', x_2' \in \mathbb{R}^c$, e.g. 
\begin{align*}
    x_1' = x_{1,2}^* \quad\quad
    x_2' = x_{1,2}^*
\end{align*}
in order to recreate the pre-merge amount of tokens.\\
This naive approach does lose information because the now unmerged tokens both have the average of their previous values, but this loss is small due their already high similarity before the merge.\\ Further exploration might yield improvements here.

\subsection{Bipartite-Soft-Matching}


\subsection{Token similarity}

\subsection{Different approaches}
Text.





\newpage
\section{Experimental procedures}
How does it differ from the original paper?

\subsection{Setup}
Text.

\subsection{Adjustments}
Creating my own fork of pytorch-fid \cite{Seitzer2020FID} to accommodate for the specific needs of this experiment. Or do I?

\subsection{Results}
Text.

\subsection{Comparison to original results}
Text.




\newpage
\section{Further Exploration}
Text.




\newpage
\section{Conclusion}
Text.





\newpage
\bibliographystyle{plain}
\bibliography{refs}


\end{document}
